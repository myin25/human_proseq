import bpnetlite

class BPNetSaveable(BPNet):
	def __init__(self, n_filters=64, n_layers=8, n_outputs=2, 
		n_control_tracks=2, alpha=1, profile_output_bias=True, 
		count_output_bias=True, name=None, trimming=None, verbose=True):
		super().__init__(n_filters, n_layers, n_outputs, n_control_tracks, alpha, profile_output_bias, count_output_bias, 
						name, trimming, verbose)
    
    def fit(self, training_data, optimizer, X_valid=None, X_ctl_valid=None, 
		y_valid=None, max_epochs=100, batch_size=64, validation_iter=100, 
		early_stopping=None, verbose=True):
		"""Fit the model to data and validate it periodically.

		This method controls the training of a BPNet model. It will fit the
		model to examples generated by the `training_data` DataLoader object
		and, if validation data is provided, will periodically validate the
		model against it and return those values. The periodicity can be
		controlled using the `validation_iter` parameter.

		Two versions of the model will be saved: the best model found during
		training according to the validation measures, and the final model
		at the end of training. Additionally, a log will be saved of the
		training and validation statistics, e.g. time and performance.


		Parameters
		----------
		training_data: torch.utils.data.DataLoader
			A generator that produces examples to train on. If n_control_tracks
			is greater than 0, must product two inputs, otherwise must produce
			only one input.

		optimizer: torch.optim.Optimizer
			An optimizer to control the training of the model.

		X_valid: torch.tensor or None, shape=(n, 4, 2114)
			A block of sequences to validate on periodically. If None, do not
			perform validation. Default is None.

		X_ctl_valid: torch.tensor or None, shape=(n, n_control_tracks, 2114)
			A block of control sequences to validate on periodically. If
			n_control_tracks is None, pass in None. Default is None.

		y_valid: torch.tensor or None, shape=(n, n_outputs, 1000)
			A block of signals to validate against. Must be provided if
			X_valid is also provided. Default is None.

		max_epochs: int
			The maximum number of epochs to train for, as measured by the
			number of times that `training_data` is exhausted. Default is 100.

		batch_size: int
			The number of examples to include in each batch. Default is 64.

		validation_iter: int
			The number of batches to train on before validating against the
			entire validation set. When the validation set is large, this
			enables the total validating time to be small compared to the
			training time by only validating periodically. Default is 100.

		early_stopping: int or None
			Whether to stop training early. If None, continue training until
			max_epochs is reached. If an integer, continue training until that
			number of `validation_iter` ticks has been hit without improvement
			in performance. Default is None.

		verbose: bool
			Whether to print out the training and evaluation statistics during
			training. Default is True.
		"""

		if X_valid is not None:
			X_valid = X_valid.cuda()
			y_valid_counts = y_valid.sum(dim=2)

		if X_ctl_valid is not None:
			X_ctl_valid = X_ctl_valid.cuda()


		iteration = 0
		early_stop_count = 0
		best_loss = float("inf")
		self.logger.start()

		for epoch in range(max_epochs):
			tic = time.time()

			for data in training_data:
				if len(data) == 3:
					X, X_ctl, y = data
					X, X_ctl, y = X.cuda(), X_ctl.cuda(), y.cuda()
				else:
					X, y = data
					X, y = X.cuda(), y.cuda()
					X_ctl = None

				# Clear the optimizer and set the model to training mode
				optimizer.zero_grad()
				self.train()

				# Run forward pass
				y_profile, y_counts = self(X, X_ctl)
				y_profile = y_profile.reshape(y_profile.shape[0], -1)
				y_profile = torch.nn.functional.log_softmax(y_profile, dim=-1)
				
				y = y.reshape(y.shape[0], -1)

				# Calculate the profile and count losses
				profile_loss = MNLLLoss(y_profile, y).mean()
				count_loss = log1pMSELoss(y_counts, y.sum(dim=-1).reshape(-1, 1)).mean()

				# Extract the profile loss for logging
				profile_loss_ = profile_loss.item()
				count_loss_ = count_loss.item()

				# Mix losses together and update the model
				loss = profile_loss + self.alpha * count_loss
				loss.backward()
				optimizer.step()

				# Report measures if desired
				if verbose and iteration % validation_iter == 0:
					train_time = time.time() - tic

					with torch.no_grad():
						self.eval()

						tic = time.time()
						y_profile, y_counts = self.predict(X_valid, X_ctl_valid)

						z = y_profile.shape
						y_profile = y_profile.reshape(y_profile.shape[0], -1)
						y_profile = torch.nn.functional.log_softmax(y_profile, dim=-1)
						y_profile = y_profile.reshape(*z)

						measures = calculate_performance_measures(y_profile, 
							y_valid, y_counts, kernel_sigma=7, 
							kernel_width=81, measures=['profile_mnll', 
							'profile_pearson', 'count_pearson', 'count_mse'])

						profile_corr = measures['profile_pearson']
						count_corr = measures['count_pearson']
						
						valid_loss = measures['profile_mnll'].mean()
						valid_loss += self.alpha * measures['count_mse'].mean()
						valid_time = time.time() - tic

						self.logger.add([epoch, iteration, train_time, 
							valid_time, profile_loss_, count_loss_, 
							measures['profile_mnll'].mean().item(), 
							numpy.nan_to_num(profile_corr).mean(),
							numpy.nan_to_num(count_corr).mean(), 
							measures['count_mse'].mean().item(),
							(valid_loss < best_loss).item()])

						self.logger.save("{}.log".format(self.name))

						if valid_loss < best_loss:
							torch.save(self, "{}.torch".format(self.name))
							best_loss = valid_loss
							early_stop_count = 0
						else:
							early_stop_count += 1

				if early_stopping is not None and early_stop_count >= early_stopping:
					break

				iteration += 1

			if early_stopping is not None and early_stop_count >= early_stopping:
				break

		torch.save(self, "{}.final.torch".format(self.name))